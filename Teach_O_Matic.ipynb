{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cbh123/teach-o-matic/blob/main/Teach_O_Matic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcRrbtmSkoVR"
      },
      "source": [
        "# üìö Teach-O-Matic\n",
        "\n",
        "In this notebook, we'll create instructional \"how to videos\" on any topic, using LangChain, OpenAI, and Replicate. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "toc",
        "id": "kLOpTMwnlCKH"
      },
      "source": [
        ">[üìö Teach-O-Matic](#scrollTo=tcRrbtmSkoVR)\n",
        "\n",
        ">[üåê Links](#scrollTo=2LUvZTDzE-4h)\n",
        "\n",
        ">[üíæ Install Stuff](#scrollTo=OniyJ8XRWQbY)\n",
        "\n",
        ">[‚õìÔ∏è Create our LangChain](#scrollTo=Ou-40KJur3ZI)\n",
        "\n",
        ">[üëü Run the Chain!](#scrollTo=CoNEpnWQFRxp)\n",
        "\n",
        ">[‚è≥ Wait for our async predictions to complete](#scrollTo=Znto050hQCjM)\n",
        "\n",
        ">[ü™° Stitch them all together!](#scrollTo=wkvtnprozZyQ)\n",
        "\n",
        ">[üçø Watch the video](#scrollTo=YWwsGXaVhgwF)\n",
        "\n",
        ">[Things we learned:](#scrollTo=or_M30Ck7Hxb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üåê Links"
      ],
      "metadata": {
        "id": "2LUvZTDzE-4h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [LangChain](https://python.langchain.com/en/latest/), for chaining together our model requests.\n",
        "- [Replicate](https://replicate.com), for running:\n",
        "  - [suno-ai/bark](https://github.com/suno-ai/bark) for creating the narrator\n",
        "  - [damo-text-to-video](https://replicate.com/cjwbw/damo-text-to-video), to create the videos\n",
        "  - [stable diffusion](https://replicate.com/stability-ai/stable-diffusion), for creating the title/ending screen image\n",
        "  - [riffusion](https://replicate.com/riffusion/riffusion), for creating the background music\n",
        "- [OpenAI Docs](https://beta.openai.com) for creating the script + video descriptions\n",
        "- [MoviePy](https://zulko.github.io/moviepy/) for stitching the video and audio together\n",
        "- [Teach-O-Matic Youtube Channel](https://www.youtube.com/@teach-o-matic). Tweet us [@replicatehq](https://twitter.com/replicatehq) and we'll add your how to video to the channel!"
      ],
      "metadata": {
        "id": "DD82tZvOc5hU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OniyJ8XRWQbY"
      },
      "source": [
        "# üíæ Install Stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I85vONvgkvEq"
      },
      "outputs": [],
      "source": [
        "!pip install replicate\n",
        "!pip install requests\n",
        "!pip install openai\n",
        "!pip install langchain\n",
        "!pip install moviepy\n",
        "from google.colab import output\n",
        "output.clear()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eEK6sxN5kYE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import time\n",
        "import numpy as np\n",
        "import replicate "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIr-fPFn_FcV"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain, SequentialChain, TransformChain\n",
        "from langchain.llms import OpenAI, Replicate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OUg8ixqown0",
        "outputId": "3dc78c83-4b35-4226-e3ee-5ffa937d025f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ],
      "source": [
        "# get your token from https://replicate.com/account\n",
        "from getpass import getpass\n",
        "\n",
        "REPLICATE_API_TOKEN = getpass()\n",
        "os.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVu7s3DF5mMa",
        "outputId": "f4e9efd6-6546-478a-821e-97abd71bbd22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ],
      "source": [
        "# get your key from https://platform.openai.com/account/api-keys\n",
        "OPENAI_API_KEY = getpass()\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou-40KJur3ZI"
      },
      "source": [
        "# ‚õìÔ∏è Create our LangChain\n",
        "First let's generate the script, video descriptions, and title for our how-to video. [LangChain](https://langchain.readthedocs.io/) makes this easy, because we can separate each LLM call into a composable chain.\n",
        "\n",
        "In this step we want to accomplish three things:\n",
        "1. a script that our narrator ([Bark](https://replicate.com/suno-ai/bark)) will read\n",
        "2. a title for our video\n",
        "3. visual descriptions for our script that we run in the [text-to-video](https://replicate.com/cjwbw/damo-text-to-video) model\n",
        "\n",
        "Down the road, we may want to add new outputs, like background music, new narrators, or images. Adding those extra outputs is easy with LangChain ‚Äî‚Äî all it requires is adding a new LLMChain.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8DvQ8Ny_mtM"
      },
      "outputs": [],
      "source": [
        "# LLMChain to write a a script for our how-to video.\n",
        "topic_template = \"\"\"\n",
        "Write me the instructions for {topic}. \n",
        "\n",
        "Rules\n",
        "- The output should 6 paragraphs, each no more than 20 words. \n",
        "- Split each output with a \\n\\n\n",
        "- Omit the word 'paragraph'. \n",
        "- The first paragraph should introduce some background on why the problem is worth solving.\n",
        "\n",
        "You are a {narrator_adjectives} narrator.\n",
        "\"\"\"\n",
        "\n",
        "system_message_prompt = SystemMessage(content=\"You are a helpful assistant that enthusiastically teaches people new topics.\")\n",
        "human_message_prompt = HumanMessagePromptTemplate(prompt=PromptTemplate(\n",
        "                                                  template=topic_template,\n",
        "                                                  input_variables=[\"topic\", \"narrator_adjectives\"]))\n",
        "\n",
        "# create the initial script\n",
        "chat = ChatOpenAI(temperature=0.9, model=\"gpt-4\")\n",
        "chat_prompt_template = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "script_chain = LLMChain(llm=chat, prompt=chat_prompt_template, output_key='script')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zUi7iQ7OGfS"
      },
      "outputs": [],
      "source": [
        "# LLMChain to write a title for our video\n",
        "llm = OpenAI(temperature=.9)\n",
        "template = \"\"\"Please come up with a creative and zany title for the below how-to video script. \n",
        "Puns are encouraged. Don't include quotations (\") in the output.\n",
        "\n",
        "Script:\n",
        "{script}\n",
        "Title: \"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"script\"], template=template)\n",
        "title_chain = LLMChain(llm=llm, prompt=prompt_template, output_key='title')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifVZOyErEILX"
      },
      "outputs": [],
      "source": [
        "# LLMChain to write a descriptions of each video that we'll create using a text to video model.\n",
        "video_template = \"\"\"\n",
        "          Create a visual description for each of the 6 following paragraphs. Keep each one as concrete as possible. \n",
        "          \n",
        "          Rules\n",
        "          - Each description should be no more than 20 words. \n",
        "          - It's good the for output to be very specific and include a single action. \n",
        "          - Do not include numbers in the output.\n",
        "          - Split each output with a \\n\\n\n",
        "\n",
        "          Example input:\n",
        "          Driving a car provides independence and convenience.\\n\\nAdjust seat, mirrors, and steering wheel for comfort.\\n\\nFamiliarize yourself with pedals, gearshift, and dashboard controls.\\n\\nStart the car, press brake, and shift gears.\\n\\nObserve traffic rules, signals, and road signs attentively.\\n\\nPractice regularly to build confidence and improve skills.\n",
        "\n",
        "          Example output:\n",
        "          Happy driver driving a car.\\n\\nDriver adjusting seat and mirrors in car.\\n\\nDriver studying pedals, gearshift, and dashboard.\\n\\nStarting car, pressing brake, and shifting gears.\\n\\nDriver obeying traffic rules and road signs.\\n\\nCar driving on road.\n",
        "          \n",
        "          \"{script}\"\"\n",
        "          \"\"\"\n",
        "\n",
        "system_message_prompt = SystemMessage(content=\"You are a helpful assistant that is helping us create visual depictions of text. Sometimes it's important to see the person doing the action. For example, cooking spaghetti may involve a chef. Fixing a car may involve a mechanic.\")\n",
        "human_message_prompt = HumanMessagePromptTemplate(prompt=PromptTemplate(\n",
        "                                                  template=video_template,\n",
        "                                                  input_variables=[\"script\"]\n",
        "                                                  ))\n",
        "\n",
        "# create the video descriptions\n",
        "chat_prompt_template = ChatPromptTemplate(messages=[system_message_prompt, human_message_prompt], input_variables=[\"script\"], partial_variables={\"format_instructions\": \"Your response should be a list of \\n separated values, eg: `foo\\nbar\\nbaz`\"})\n",
        "video_descriptions_chain = LLMChain(llm=chat, prompt=chat_prompt_template, output_key='video_descriptions')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fl38V4HSISrl"
      },
      "outputs": [],
      "source": [
        "# let's define a new output parser!\n",
        "from langchain.output_parsers.list import ListOutputParser\n",
        "from typing import List\n",
        "\n",
        "class DoubleNewlineOutputParser(ListOutputParser):\n",
        "  \"\"\"Parse out \\n\\n separated lists.\"\"\"\n",
        "  def get_format_instructions(self) -> str:\n",
        "      return (\n",
        "          \"Your response should be a list of \\n\\n separated values, \"\n",
        "          \"eg: `foo\\n\\nbar\\n\\nbaz`\"\n",
        "      )\n",
        "\n",
        "  def parse(self, text: str) -> List[str]:\n",
        "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
        "        return text.strip().split(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GH99FV1x-agB"
      },
      "outputs": [],
      "source": [
        "# LLMChain to create the replicate predictions for our text-to-video model\n",
        "def transform_func(inputs: dict) -> dict:\n",
        "  video_model = replicate.models.get('cjwbw/damo-text-to-video')\n",
        "  video_version = video_model.versions.get(\"1e205ea73084bd17a0a3b43396e49ba0d6bc2e754e9283b2df49fad2dcf95755\")\n",
        "  descriptions = DoubleNewlineOutputParser().parse(inputs['video_descriptions'])\n",
        "  \n",
        "  predictions = []\n",
        "\n",
        "  for description in descriptions:\n",
        "      print(f\"Creating video prediction for '{description}'...\")\n",
        "      video_prediction = replicate.predictions.create(version=video_version, \n",
        "                                                      input={\"prompt\": description, \"num_frames\": 50, \"fps\": 10}) # 5s videos\n",
        "      predictions.append(video_prediction)\n",
        "  return {'video_predictions': predictions}\n",
        "\n",
        "video_predictions_chain = TransformChain(input_variables=['video_descriptions'], output_variables=['video_predictions'], transform=transform_func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtPhTZwjEKHP"
      },
      "outputs": [],
      "source": [
        "# LLMChain to create the replicate predictions for our bark model\n",
        "def transform_func(inputs: dict) -> dict:\n",
        "  audio_model = replicate.models.get(\"suno-ai/bark\")\n",
        "  audio_version = audio_model.versions.get(\"b76242b40d67c76ab6742e987628a2a9ac019e11d56ab96c4e91ce03b79b2787\")\n",
        "  parsed_script = DoubleNewlineOutputParser().parse(inputs['script'])\n",
        "  \n",
        "  predictions = []\n",
        "\n",
        "  for line in parsed_script:\n",
        "      print(f\"Creating audio prediction for '{line}''...\")\n",
        "      audio_prediction = replicate.predictions.create(version=audio_version, \n",
        "                                                      input={\"prompt\": line, \"history_prompt\": \"announcer\"})\n",
        "      predictions.append(audio_prediction)\n",
        "  return {'audio_predictions': predictions}\n",
        "\n",
        "audio_predictions_chain = TransformChain(input_variables=['script'], output_variables=['audio_predictions'], transform=transform_func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucZDP__PVl0C"
      },
      "outputs": [],
      "source": [
        "# LLMChain to create the cover image\n",
        "llm = OpenAI(temperature=.9)\n",
        "template = \"\"\"\n",
        "          Create a visual description for the following script. \n",
        "          \"{script}\"\"\n",
        "          \"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"script\"], template=template)\n",
        "text2image = Replicate(model=\"stability-ai/stable-diffusion:db21e45d3f7023abc2a46ee38a23973f6dce16bb082a930b0c49861f96d1e5bf\", \n",
        "                       input={'image_dimensions': '512x512', \"negative_prompt\": \"text, writing\"})\n",
        "title_image_chain = LLMChain(llm=text2image, prompt=prompt_template, output_key='title_image')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvZ-IHRoUpqb"
      },
      "outputs": [],
      "source": [
        "# LLMChain to write the thank you note at the end of our video\n",
        "llm = OpenAI(temperature=.9)\n",
        "template = \"\"\"Please come up with a creative and zany ending quote from our narrator. \n",
        "The script is what the narrator just read. We want to close things out.\n",
        "\n",
        "Script:\n",
        "{script}\n",
        "Ending quote: \n",
        "\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"script\"], template=template)\n",
        "ending_quote_chain = LLMChain(llm=llm, prompt=prompt_template, output_key='ending_quote')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nTkKwQTY38m"
      },
      "outputs": [],
      "source": [
        "# LLMChain to create the prediction that generates the audio for the thank you note\n",
        "def transform_func(inputs: dict) -> dict:\n",
        "  audio_model = replicate.models.get(\"suno-ai/bark\")\n",
        "  audio_version = audio_model.versions.get(\"b76242b40d67c76ab6742e987628a2a9ac019e11d56ab96c4e91ce03b79b2787\")\n",
        "  ending_quote_prediction = replicate.predictions.create(version=audio_version, \n",
        "                                                      input={\"prompt\": inputs['ending_quote'], \"history_prompt\": \"announcer\"})\n",
        "  return {'ending_quote_prediction': ending_quote_prediction}\n",
        "\n",
        "ending_quote_prediction_chain = TransformChain(input_variables=['ending_quote'], output_variables=['ending_quote_prediction'], transform=transform_func)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkAkJofuhxe1"
      },
      "outputs": [],
      "source": [
        "# LLMChain to create the prediction that generates the audio for the title\n",
        "def transform_func(inputs: dict) -> dict:\n",
        "  audio_model = replicate.models.get(\"suno-ai/bark\")\n",
        "  audio_version = audio_model.versions.get(\"b76242b40d67c76ab6742e987628a2a9ac019e11d56ab96c4e91ce03b79b2787\")\n",
        "  title_prediction = replicate.predictions.create(version=audio_version, \n",
        "                                                      input={\"prompt\": inputs['title'], \"history_prompt\": \"announcer\"})\n",
        "  return {'title_audio_prediction': title_prediction}\n",
        "\n",
        "title_audio_prediction_chain = TransformChain(input_variables=['title'], output_variables=['title_audio_prediction'], transform=transform_func)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqmmk5AaNJjg"
      },
      "outputs": [],
      "source": [
        "# LLMChain to create the prediction that generates the background music\n",
        "def transform_func(inputs: dict) -> dict:\n",
        "  model = replicate.models.get(\"riffusion/riffusion\")\n",
        "  version = model.versions.get(\"8cf61ea6c56afd61d8f5b9ffd14d7c216c0a93844ce2d82ac1c9ecc9c7f24e05\")\n",
        "  music_prediction = replicate.predictions.create(version=version, input={\"prompt\": inputs['music_style']})\n",
        "\n",
        "  return {'music_prediction': music_prediction}\n",
        "\n",
        "music_prediction_chain = TransformChain(input_variables=['music_style'], output_variables=['music_prediction'], transform=transform_func)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoNEpnWQFRxp"
      },
      "source": [
        "# üëü Run the Chain!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQwdZXW9aUQd"
      },
      "source": [
        "> ‚¨áÔ∏è We're about to run our LangChain. Make sure to change the constants below to customize the how-to video!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "4ff0GBI1aCsW"
      },
      "outputs": [],
      "source": [
        "TOPIC = \"how to fly a plane\" # make sure it has \"how to _\"\n",
        "NARRATOR_ADJECTIVES = \"rhyming\"\n",
        "MUSIC_STYLE = \"epic intense\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqsflCZCa1-j"
      },
      "source": [
        "> ‚¨ÜÔ∏è Make sure to change the constants above to customize the how-to video!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5MHzwYUE95h",
        "outputId": "5141a9f5-c807-4204-96ff-20baa7d4de8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "Creating video prediction for 'Pilot standing proudly near airplane.'...\n",
            "Creating video prediction for 'Pilot holding a checklist near plane.'...\n",
            "Creating video prediction for 'Plane taxiing gracefully on a runway.'...\n",
            "Creating video prediction for 'Airplane accelerating speed and taking off.'...\n",
            "Creating video prediction for 'Pilot navigating and focusing on route.'...\n",
            "Creating video prediction for 'Smooth landing of the airplane.'...\n",
            "Creating audio prediction for 'Flying a plane, a skill so divine,\n",
            "Conquering skies, where dreams entwine.''...\n",
            "Creating audio prediction for 'Start your journey, checklist in hand,\n",
            "Ensure all's well, then take a stand.''...\n",
            "Creating audio prediction for 'Taxi, line up, on the runway with grace,\n",
            "Throttle up, let the speed embrace.''...\n",
            "Creating audio prediction for 'Rotate the bird, to the sky ascend,\n",
            "Lift-off achieved, fear you'll now transcend.''...\n",
            "Creating audio prediction for 'Navigate the route, with vigilance, my friend,\n",
            "For at journey's end, safely descend.''...\n",
            "Creating audio prediction for 'Touchdown smoothly, gently with might,\n",
            "Congratulations pilot, you've conquered the flight.''...\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Run the chain\n",
        "overall_chain = SequentialChain(chains=[script_chain, \n",
        "                                        title_chain, \n",
        "                                        video_descriptions_chain, \n",
        "                                        video_predictions_chain, \n",
        "                                        audio_predictions_chain,\n",
        "                                        ending_quote_chain,\n",
        "                                        ending_quote_prediction_chain,\n",
        "                                        title_image_chain,\n",
        "                                        title_audio_prediction_chain,\n",
        "                                        music_prediction_chain\n",
        "                                        ], input_variables=['topic', 'narrator_adjectives', 'music_style'], output_variables=['script', 'video_descriptions', 'title', 'video_predictions', 'audio_predictions', 'ending_quote', 'title_image', 'ending_quote_prediction', 'title_audio_prediction', 'music_prediction'], verbose=True)\n",
        "chain_output = overall_chain({\"topic\": TOPIC, \"narrator_adjectives\": NARRATOR_ADJECTIVES, \"music_style\": MUSIC_STYLE})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "6_hvy--bLtl-"
      },
      "outputs": [],
      "source": [
        "# unpack outputs\n",
        "title = chain_output['title']\n",
        "script = chain_output['script']\n",
        "split_script = script.split('\\n\\n')\n",
        "video_descriptions = chain_output['video_descriptions'].split('\\n\\n')\n",
        "video_predictions = chain_output['video_predictions']\n",
        "audio_predictions = chain_output['audio_predictions']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "chcuZQGx6xjr"
      },
      "outputs": [],
      "source": [
        "# sanity check\n",
        "assert len(split_script) == 6\n",
        "assert len(video_descriptions) == 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Znto050hQCjM"
      },
      "source": [
        "# ‚è≥ Wait for our async predictions to complete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "ChI2MpOfQJWs"
      },
      "outputs": [],
      "source": [
        "def all_done(predictions):\n",
        "    return set([p.status for p in predictions]) == {'succeeded'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "Y4PwZ_oZf9Ks"
      },
      "outputs": [],
      "source": [
        "all_predictions = chain_output['video_predictions'] + \\\n",
        "                  chain_output['audio_predictions'] + \\\n",
        "                  [chain_output['ending_quote_prediction']] + \\\n",
        "                  [chain_output['title_audio_prediction']] + \\\n",
        "                  [chain_output['music_prediction']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUkZmw0iQIGR",
        "outputId": "2ee3bd9b-9474-4631-d9a0-9d2763cc179b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://replicate.com/p/joj5qqxlkvfjjaijqep35tx53i succeeded\n",
            "https://replicate.com/p/farlixlflvawthlqgvezuh5afe succeeded\n",
            "https://replicate.com/p/uqhuwnml7fh4vm5epc75q3auje succeeded\n",
            "https://replicate.com/p/7tgkgsaiwjc6blvaiy27bfz76y succeeded\n",
            "https://replicate.com/p/pydn3erfofbkphpl3zzznn3emq succeeded\n",
            "https://replicate.com/p/yophclw65vh65ntqha6i6kdzwu succeeded\n",
            "https://replicate.com/p/y4lqc4ywobhwjpnxv6jvukcl24 succeeded\n",
            "https://replicate.com/p/qea7i7ef5vajjha4ppfcwjoaiq succeeded\n",
            "https://replicate.com/p/zinc2jtkbzdd7kpciug3nu7zvu succeeded\n",
            "https://replicate.com/p/esrox5beszbi7edtdzwxxjmhlu processing\n",
            "https://replicate.com/p/oushwqss7rdgnjbh3xjmxuabmi processing\n",
            "https://replicate.com/p/gp6fqmik5renbgrq4bulcrqugi processing\n",
            "https://replicate.com/p/lfvlesuux5fxfejgt62rf7zp2q processing\n",
            "https://replicate.com/p/343ms2bemzar3jairjac6vagdi starting\n",
            "https://replicate.com/p/p6wrpiij3je5dnp2wthfo63mu4 succeeded\n"
          ]
        }
      ],
      "source": [
        "done = False\n",
        "\n",
        "while not done:\n",
        "  [p.reload() for p in all_predictions]\n",
        "  for p in all_predictions:\n",
        "    print(f'https://replicate.com/p/{p.id}', p.status)\n",
        "  done = all_done(all_predictions)\n",
        "  time.sleep(2)\n",
        "  output.clear()\n",
        "\n",
        "print(\"Predictions complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkvtnprozZyQ"
      },
      "source": [
        "# ü™° Stitch them all together!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZnV3CMGP8Yc"
      },
      "outputs": [],
      "source": [
        "video_urls = [v.output for v in video_predictions]\n",
        "audio_urls = [a.output['audio_out'] for a in audio_predictions]\n",
        "music_url = chain_output['music_prediction'].output['audio']\n",
        "subtitles = split_script\n",
        "title_image_url = chain_output['title_image']\n",
        "title_audio_url = chain_output['title_audio_prediction'].output['audio_out']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUEDoBPlTHAd"
      },
      "source": [
        "Thank you to GPT-4 for this code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6Nw3oggwTi9"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import os\n",
        "import moviepy.editor as mp\n",
        "import moviepy.video.fx.all as vfx\n",
        "import textwrap\n",
        "from moviepy.editor import *\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from io import BytesIO\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Download video and audio files\n",
        "video_files = []\n",
        "audio_files = []\n",
        "for i, url in enumerate(video_urls):\n",
        "    response = requests.get(url)\n",
        "    video_filename = f\"temp_video{i}.mp4\"\n",
        "    with open(video_filename, \"wb\") as video_file:\n",
        "        video_file.write(response.content)\n",
        "    video_files.append(video_filename)\n",
        "\n",
        "for i, url in enumerate(audio_urls):\n",
        "    response = requests.get(url)\n",
        "    audio_filename = f\"temp_audio{i}.mp3\"\n",
        "    with open(audio_filename, \"wb\") as audio_file:\n",
        "        audio_file.write(response.content)\n",
        "    audio_files.append(audio_filename)\n",
        "\n",
        "# Load and process video and audio files\n",
        "processed_videos = []\n",
        "for i, audio_file in enumerate(audio_files):\n",
        "    video = mp.VideoFileClip(video_files[i])\n",
        "    audio = mp.AudioFileClip(audio_file)\n",
        "\n",
        "    # Loop the video for the duration of the audio\n",
        "    looped_video = mp.concatenate_videoclips([video] * int(audio.duration // video.duration + 1))\n",
        "\n",
        "    # Set the audio of the video to the audio file\n",
        "    video_with_audio = looped_video.set_audio(audio)\n",
        "    processed_videos.append(video_with_audio)\n",
        "\n",
        "# Concatenate all the processed videos\n",
        "final_video = mp.concatenate_videoclips(processed_videos)\n",
        "\n",
        "## The following adds the title image / narration to the video.\n",
        "# Add this function to create the text image\n",
        "def txt_image(img, txt, font_size, color):\n",
        "    image = img.copy()\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    draw.text((50, 50), txt, fill=(255, 255, 0))  \n",
        "    # font = ImageFont.load_default().font_variant(size=font_size)\n",
        "    # draw.text((50, 50), txt, font=font, fill=color)\n",
        "    return image\n",
        "\n",
        "# Download and create the image clip\n",
        "image_url = title_image_url\n",
        "response = requests.get(image_url)\n",
        "img = Image.open(BytesIO(response.content))\n",
        "\n",
        "# Resize the image to match the video dimensions\n",
        "img_resized = img.resize((1200, 900))\n",
        "\n",
        "# Download the audio file\n",
        "audio_url = chain_output['ending_quote_prediction'].output['audio_out']\n",
        "response = requests.get(audio_url)\n",
        "with open(\"temp_audio_ending.mp3\", \"wb\") as audio_file:\n",
        "    audio_file.write(response.content)\n",
        "\n",
        "# Create the audio clip\n",
        "audio_ending = AudioFileClip(\"temp_audio_ending.mp3\")\n",
        "\n",
        "# make title empty for now, couldn't figure out how to get it bigger\n",
        "text = chain_output['title']\n",
        "img_text = ImageClip(np.asarray(txt_image(img_resized, txt=\"\", font_size=48, color=\"white\")), duration=4)\n",
        "\n",
        "# Set the audio of the image clip to the audio file and trim it to the same duration\n",
        "img_text_audio_ending = mp.concatenate_videoclips([img_text] * int(audio_ending.duration // img_text.duration + 1))\n",
        "img_text_audio_ending = img_text.set_audio(audio_ending)\n",
        "\n",
        "# Download the title page audio file\n",
        "audio_url = chain_output['title_audio_prediction'].output['audio_out']\n",
        "response = requests.get(audio_url)\n",
        "with open(\"temp_audio_title.mp3\", \"wb\") as audio_file:\n",
        "    audio_file.write(response.content)\n",
        "\n",
        "# Create the audio clip\n",
        "audio_beginning = AudioFileClip(\"temp_audio_title.mp3\")\n",
        "\n",
        "# Set the audio of the image clip to the audio file and trim it to the same duration\n",
        "img_text_audio_beginning = mp.concatenate_videoclips([img_text] * int(audio_beginning.duration // img_text.duration + 1))\n",
        "img_text_audio_beginning = img_text.set_audio(audio_beginning)\n",
        "\n",
        "# Concatenate the image clip with the processed videos\n",
        "width, height = processed_videos[0].size\n",
        "title_video = img_text_audio_beginning.resize((width, height))\n",
        "ending_video = img_text_audio_ending.resize((width, height))\n",
        "\n",
        "processed_videos.insert(0, title_video)\n",
        "processed_videos.append(ending_video)\n",
        "\n",
        "final_video = concatenate_videoclips(processed_videos)\n",
        "\n",
        "# Download the background audio file\n",
        "bg_audio_url = music_url\n",
        "response = requests.get(bg_audio_url)\n",
        "with open(\"temp_bg_audio.mp3\", \"wb\") as audio_file:\n",
        "    audio_file.write(response.content)\n",
        "\n",
        "# Create the background audio clip\n",
        "bg_audio = AudioFileClip(\"temp_bg_audio.mp3\")\n",
        "\n",
        "# Calculate the duration of the final video\n",
        "video_duration = final_video.duration\n",
        "\n",
        "# Loop the background audio to match the final video's duration\n",
        "bg_audio_looped = bg_audio.fx(afx.audio_loop, duration=video_duration)\n",
        "bg_audio_looped = bg_audio_looped.volumex(0.5)\n",
        "\n",
        "# Overlay the background audio with the audio from the final video\n",
        "final_audio = CompositeAudioClip([final_video.audio, bg_audio_looped])\n",
        "\n",
        "# Set the audio of the final video to the combined audio\n",
        "final_video_with_bg_audio = final_video.set_audio(final_audio)\n",
        "\n",
        "# Save the final video\n",
        "final_video_with_bg_audio.write_videofile(f\"how_to.mp4\", codec='libx264', audio_codec='aac')\n",
        "\n",
        "# Clean up temporary files\n",
        "for video_file, audio_file in zip(video_files, audio_files):\n",
        "    os.remove(video_file)\n",
        "for audio_file in audio_files:\n",
        "    os.remove(audio_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWwsGXaVhgwF"
      },
      "source": [
        "# üçø Watch the video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V30X3XRxyo_V"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open(f'how_to.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or_M30Ck7Hxb"
      },
      "source": [
        "# üß† Things we learned:\n",
        "- It's hard to get the open source base LLMs like dolly/stable-lm to respond with structured text (like a consistent 3 paragraph response)\n",
        "- You can use SequentialChain in LangChain to handle multiple inputs and outputs\n",
        "- You can use a TransformChain to run arbitrary functions in your LangChain. This is how I'm running multiple async Replicate calls\n",
        "- Sometimes the damo/text-to-video output is fuzzy. Especially when the video description is vague. So it's really important that our LLM has a concrete description for what the video should be! It would be really cool if our LLM was multi-modal so we could feed back the video output and say \"if this is fuzzy can you make the description more concrete and try again?\"\n",
        "- GPT-4 was really helpful for stitching all the clips together! The final product is combining images, videos, narration clips, and background music (at 50% volume), and it all sort of works.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kv7v8XL27KMw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}